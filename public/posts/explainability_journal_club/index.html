<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Evaluation of Explainable AI Frameworks in Healthcare | Pharmacy AI Newsletter</title>
<meta name="keywords" content="">
<meta name="description" content="Evaluation of multiple explainable AI methods (local and global) in healthcare using the XAI-Eval framework to assess fidelity, stability, complexity, and trustworthiness of clinical predictions.">
<meta name="author" content="">
<link rel="canonical" href="https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/posts/explainability_journal_club/">
<link crossorigin="anonymous" href="/Pharmacy-AI-Newsletter/assets/css/stylesheet.37722ee1e8e48e0d2e8c4a171f1904408ac86675a9b1dc1260db07996f76f425.css" integrity="sha256-N3Iu4ejkjg0ujEoXHxkEQIrIZnWpsdwSYNsHmW929CU=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/apple-touch-icon.png">
<link rel="mask-icon" href="https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/posts/explainability_journal_club/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/posts/explainability_journal_club/">
  <meta property="og:site_name" content="Pharmacy AI Newsletter">
  <meta property="og:title" content="Evaluation of Explainable AI Frameworks in Healthcare">
  <meta property="og:description" content="Evaluation of multiple explainable AI methods (local and global) in healthcare using the XAI-Eval framework to assess fidelity, stability, complexity, and trustworthiness of clinical predictions.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-04T11:34:58-04:00">
    <meta property="article:modified_time" content="2025-10-04T11:34:58-04:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Evaluation of Explainable AI Frameworks in Healthcare">
<meta name="twitter:description" content="Evaluation of multiple explainable AI methods (local and global) in healthcare using the XAI-Eval framework to assess fidelity, stability, complexity, and trustworthiness of clinical predictions.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Evaluation of Explainable AI Frameworks in Healthcare",
      "item": "https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/posts/explainability_journal_club/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Evaluation of Explainable AI Frameworks in Healthcare",
  "name": "Evaluation of Explainable AI Frameworks in Healthcare",
  "description": "Evaluation of multiple explainable AI methods (local and global) in healthcare using the XAI-Eval framework to assess fidelity, stability, complexity, and trustworthiness of clinical predictions.",
  "keywords": [
    
  ],
  "articleBody": "Clinical/Operational Question Question: In healthcare machine learning, how do widely used explanation techniques (local and global) compare in their ability to provide trustworthy, useful explanations for clinical predictions, and how can we systematically evaluate these methods in real-world settings?\nBackground \u0026 Rationale Why it matters: AI-driven decision support in medication management and clinical care must be explainable to earn clinician trust and meet safety/regulatory requirements. Pharmacy informatics teams integrating AI (e.g., into Epic EHR alerts or dosing tools) face a critical need for transparent algorithms so pharmacists understand and validate AI recommendations before acting. Multiple eXplainable AI (XAI) methods (LIME, SHAP, Anchors, rule-based models, etc.) exist, but it’s unclear which is optimal for a given clinical task. High-stakes environments like EHR medication ordering and CDS demand rigorous evaluation of explainability – poor explanations can erode user confidence or hide biases, whereas robust explanations can improve acceptance and oversight of AI-driven alerts. This study addresses the gap by proposing a framework to quantitatively compare XAI methods on healthcare data, guiding informatics teams in choosing the right approach for safe, transparent AI deployment.\nMethods Snapshot Design: Comparative in silico evaluation of XAI techniques using retrospective clinical datasets (multiple healthcare prediction tasks). XAI Methods: Five local explainers (LIME, CIU, RuleFit, RuleMatrix, Anchors) and four global explainers (LIME, Anchors, RuleFit, RuleMatrix). Data: Diverse healthcare datasets across diagnostic and prognostic tasks. Evaluation Metrics: Fidelity, stability/robustness, complexity/simplicity, monotonicity, non-sensitivity. Analysis: Quantitative comparisons of local and global methods across datasets. Key Results No single XAI method dominated every metric. Global explanations: RuleFit and RuleMatrix were most interpretable and faithful. Local explanations: Rule-based methods excelled in stability and fidelity; CIU best at monotonicity; LIME weakest on monotonicity but avoided irrelevant features. Trade-offs: Simpler methods (LIME) offered interpretability but reduced fidelity; higher fidelity approaches tended to be more complex. Implication: Context-dependent selection is necessary based on clinical priorities. Strengths and Limitations Strengths Comprehensive head-to-head comparison of multiple methods. Structured framework with standardized metrics. Real-world healthcare datasets (not synthetic). Pragmatic, actionable guidance with no conflicts of interest. Limitations Did not include some popular methods (e.g., SHAP, counterfactuals). Lacked user/clinician validation. Retrospective only; no live EHR deployment tested. Domain-specific nuances may require re-testing. AI-Specific Considerations Transparency \u0026 Trust: Balancing clarity and completeness is key. High-fidelity and stability crucial for clinician trust. Generalizability: Rule-based global methods showed robustness, but site-specific testing required. Bias \u0026 Fairness: Explanations may reveal or obscure bias; fairness must be evaluated alongside explainability. Applicability to Pharmacy Informatics Epic Willow \u0026 Pyxis ES: Rule-based global explanations (RuleFit/RuleMatrix) can integrate as interpretable rules for pharmacists. FHIR CDS Hooks: Explanations can be embedded in alerts to improve trust. Clinical Alerting Systems: XAI-Eval framework can optimize alerts, reduce fatigue, and ensure new predictive alerts come with understandable justifications. Related Literature Synthesis Qureshi et al., 2025: SHAP achieved perfect fidelity for tree models; LIME simpler but less faithful. Reinforces that context matters. Liu et al., 2024: Applied XAI to Epic alerts, generating actionable changes that reduced ~9% of irrelevant alerts. Noor et al., 2025: Review of XAI in healthcare highlighted the need for standardized evaluation and clinical integration, echoing XAI-Eval’s importance. Evidence Quality \u0026 Recommendation Evidence Quality: Moderate certainty. Strong technical evidence across multiple datasets, but no patient outcomes. Recommendation: Strongly recommend pharmacy informatics teams adopt an evaluation framework (like XAI-Eval) when implementing AI-driven CDS, especially in medication safety contexts. Bottom Line Select explanation methods as carefully as predictive models. Use XAI-Eval or similar frameworks to vet methods before deployment. Integrate explanations into workflow (e.g., Epic alerts) to improve trust and reduce alert fatigue. Transparency = Safety: Explainable AI supports safer pharmacy decision-making. Citation Agrawal K, El Shawi R, Ahmed N. eXplainable artificial intelligence-Eval: A framework for comparative evaluation of explanation methods in healthcare. Digit Health. 2025;11:20552076251368045. DOI: 10.1177/20552076251368045\nReferences (Primary + Comparative Studies) Agrawal K, El Shawi R, Ahmed N. eXplainable artificial intelligence-Eval: A framework for comparative evaluation of explanation methods in healthcare. Digit Health. 2025;11:20552076251368045. DOI: 10.1177/20552076251368045 Qureshi MA, Noor AA, Manzoor A, et al. Explainability in Action: A Metric-Driven Assessment of Five XAI Methods for Healthcare Tabular Models. medRxiv. 2025 May 20;2025.05.20.25327976. DOI: 10.1101/2025.05.20.25327976 Liu S, McCoy AB, Peterson JF, et al. Leveraging explainable artificial intelligence to optimize clinical decision support. J Am Med Inform Assoc. 2024;31(4):968-974. DOI: 10.1093/jamia/ocae019 Noor AA, Manzoor A, Qureshi MDM, et al. Unveiling Explainable AI in Healthcare: Current Trends, Challenges, and Future Directions. Wiley Interdiscip Rev Data Min Knowl Discov. 2025;15(4):e70018. DOI: 10.1002/widm.70018 ",
  "wordCount" : "738",
  "inLanguage": "en",
  "datePublished": "2025-10-04T11:34:58-04:00",
  "dateModified": "2025-10-04T11:34:58-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/posts/explainability_journal_club/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Pharmacy AI Newsletter",
    "logo": {
      "@type": "ImageObject",
      "url": "https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/" accesskey="h" title="Pharmacy AI Newsletter (Alt + H)">Pharmacy AI Newsletter</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<nav class="post-navigation">
  <a href="https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/" class="back-to-posts">
    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <line x1="19" y1="12" x2="5" y2="12"></line>
      <polyline points="12 19 5 12 12 5"></polyline>
    </svg>
    <span>Back to Posts</span>
  </a>
</nav>

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title entry-hint-parent">
      Evaluation of Explainable AI Frameworks in Healthcare
    </h1>
    <div class="post-metadata-card">
      <div class="metadata-row">
        <span class="metadata-label">Authors:</span>
        <span class="metadata-value">Agrawal K, El Shawi R, Ahmed N.</span>
      </div>
      <div class="metadata-row">
        <span class="metadata-label">Journal:</span>
        <span class="metadata-value">Digital Health</span>
      </div>
      <div class="metadata-row">
        <span class="metadata-label">DOI:</span>
        <span class="metadata-value">
          <a href="https://doi.org/10.1177/20552076251368045" target="_blank" rel="noopener">10.1177/20552076251368045</a>
        </span>
      </div>
    </div>
    <div class="post-description">
      Evaluation of multiple explainable AI methods (local and global) in healthcare using the XAI-Eval framework to assess fidelity, stability, complexity, and trustworthiness of clinical predictions.
    </div>
    <div class="post-meta"><span title='2025-10-04 11:34:58 -0400 EDT'>October 4, 2025</span>

</div>
  </header> 
  <div class="post-content journal-club-content"><h2 id="clinicaloperational-question">Clinical/Operational Question<a hidden class="anchor" aria-hidden="true" href="#clinicaloperational-question">#</a></h2>
<p><strong>Question:</strong> In healthcare machine learning, how do widely used explanation techniques (local and global) compare in their ability to provide trustworthy, useful explanations for clinical predictions, and how can we systematically evaluate these methods in real-world settings?</p>
<hr>
<h2 id="background--rationale">Background &amp; Rationale<a hidden class="anchor" aria-hidden="true" href="#background--rationale">#</a></h2>
<p>Why it matters: AI-driven decision support in medication management and clinical care must be explainable to earn clinician trust and meet safety/regulatory requirements. Pharmacy informatics teams integrating AI (e.g., into Epic EHR alerts or dosing tools) face a critical need for transparent algorithms so pharmacists understand and validate AI recommendations before acting. Multiple eXplainable AI (XAI) methods (LIME, SHAP, Anchors, rule-based models, etc.) exist, but it’s unclear which is optimal for a given clinical task. High-stakes environments like EHR medication ordering and CDS demand rigorous evaluation of explainability – poor explanations can erode user confidence or hide biases, whereas robust explanations can improve acceptance and oversight of AI-driven alerts. This study addresses the gap by proposing a framework to quantitatively compare XAI methods on healthcare data, guiding informatics teams in choosing the right approach for safe, transparent AI deployment.</p>
<hr>
<h2 id="methods-snapshot">Methods Snapshot<a hidden class="anchor" aria-hidden="true" href="#methods-snapshot">#</a></h2>
<ul>
<li><strong>Design:</strong> Comparative in silico evaluation of XAI techniques using retrospective clinical datasets (multiple healthcare prediction tasks).</li>
<li><strong>XAI Methods:</strong> Five local explainers (LIME, CIU, RuleFit, RuleMatrix, Anchors) and four global explainers (LIME, Anchors, RuleFit, RuleMatrix).</li>
<li><strong>Data:</strong> Diverse healthcare datasets across diagnostic and prognostic tasks.</li>
<li><strong>Evaluation Metrics:</strong> Fidelity, stability/robustness, complexity/simplicity, monotonicity, non-sensitivity.</li>
<li><strong>Analysis:</strong> Quantitative comparisons of local and global methods across datasets.</li>
</ul>
<hr>
<h2 id="key-results">Key Results<a hidden class="anchor" aria-hidden="true" href="#key-results">#</a></h2>
<ul>
<li>No single XAI method dominated every metric.</li>
<li><strong>Global explanations:</strong> RuleFit and RuleMatrix were most interpretable and faithful.</li>
<li><strong>Local explanations:</strong> Rule-based methods excelled in stability and fidelity; CIU best at monotonicity; LIME weakest on monotonicity but avoided irrelevant features.</li>
<li><strong>Trade-offs:</strong> Simpler methods (LIME) offered interpretability but reduced fidelity; higher fidelity approaches tended to be more complex.</li>
<li><strong>Implication:</strong> Context-dependent selection is necessary based on clinical priorities.</li>
</ul>
<hr>
<h2 id="strengths-and-limitations">Strengths and Limitations<a hidden class="anchor" aria-hidden="true" href="#strengths-and-limitations">#</a></h2>
<h3 id="strengths">Strengths<a hidden class="anchor" aria-hidden="true" href="#strengths">#</a></h3>
<ul>
<li>Comprehensive head-to-head comparison of multiple methods.</li>
<li>Structured framework with standardized metrics.</li>
<li>Real-world healthcare datasets (not synthetic).</li>
<li>Pragmatic, actionable guidance with no conflicts of interest.</li>
</ul>
<h3 id="limitations">Limitations<a hidden class="anchor" aria-hidden="true" href="#limitations">#</a></h3>
<ul>
<li>Did not include some popular methods (e.g., SHAP, counterfactuals).</li>
<li>Lacked user/clinician validation.</li>
<li>Retrospective only; no live EHR deployment tested.</li>
<li>Domain-specific nuances may require re-testing.</li>
</ul>
<hr>
<h2 id="ai-specific-considerations">AI-Specific Considerations<a hidden class="anchor" aria-hidden="true" href="#ai-specific-considerations">#</a></h2>
<ul>
<li><strong>Transparency &amp; Trust:</strong> Balancing clarity and completeness is key. High-fidelity and stability crucial for clinician trust.</li>
<li><strong>Generalizability:</strong> Rule-based global methods showed robustness, but site-specific testing required.</li>
<li><strong>Bias &amp; Fairness:</strong> Explanations may reveal or obscure bias; fairness must be evaluated alongside explainability.</li>
</ul>
<hr>
<h2 id="applicability-to-pharmacy-informatics">Applicability to Pharmacy Informatics<a hidden class="anchor" aria-hidden="true" href="#applicability-to-pharmacy-informatics">#</a></h2>
<ul>
<li><strong>Epic Willow &amp; Pyxis ES:</strong> Rule-based global explanations (RuleFit/RuleMatrix) can integrate as interpretable rules for pharmacists.</li>
<li><strong>FHIR CDS Hooks:</strong> Explanations can be embedded in alerts to improve trust.</li>
<li><strong>Clinical Alerting Systems:</strong> XAI-Eval framework can optimize alerts, reduce fatigue, and ensure new predictive alerts come with understandable justifications.</li>
</ul>
<hr>
<h2 id="related-literature-synthesis">Related Literature Synthesis<a hidden class="anchor" aria-hidden="true" href="#related-literature-synthesis">#</a></h2>
<ul>
<li><strong>Qureshi et al., 2025:</strong> SHAP achieved perfect fidelity for tree models; LIME simpler but less faithful. Reinforces that context matters.</li>
<li><strong>Liu et al., 2024:</strong> Applied XAI to Epic alerts, generating actionable changes that reduced ~9% of irrelevant alerts.</li>
<li><strong>Noor et al., 2025:</strong> Review of XAI in healthcare highlighted the need for standardized evaluation and clinical integration, echoing XAI-Eval’s importance.</li>
</ul>
<hr>
<h2 id="evidence-quality--recommendation">Evidence Quality &amp; Recommendation<a hidden class="anchor" aria-hidden="true" href="#evidence-quality--recommendation">#</a></h2>
<ul>
<li><strong>Evidence Quality:</strong> Moderate certainty. Strong technical evidence across multiple datasets, but no patient outcomes.</li>
<li><strong>Recommendation:</strong> Strongly recommend pharmacy informatics teams adopt an evaluation framework (like XAI-Eval) when implementing AI-driven CDS, especially in medication safety contexts.</li>
</ul>
<hr>
<h2 id="bottom-line">Bottom Line<a hidden class="anchor" aria-hidden="true" href="#bottom-line">#</a></h2>
<ul>
<li>Select explanation methods as carefully as predictive models.</li>
<li>Use XAI-Eval or similar frameworks to vet methods before deployment.</li>
<li>Integrate explanations into workflow (e.g., Epic alerts) to improve trust and reduce alert fatigue.</li>
<li>Transparency = Safety: Explainable AI supports safer pharmacy decision-making.</li>
</ul>
<hr>
<h2 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h2>
<p>Agrawal K, El Shawi R, Ahmed N. eXplainable artificial intelligence-Eval: A framework for comparative evaluation of explanation methods in healthcare. Digit Health. 2025;11:20552076251368045. DOI: 10.1177/20552076251368045</p>
<hr>
<h2 id="references-primary--comparative-studies">References (Primary + Comparative Studies)<a hidden class="anchor" aria-hidden="true" href="#references-primary--comparative-studies">#</a></h2>
<ol>
<li>Agrawal K, El Shawi R, Ahmed N. eXplainable artificial intelligence-Eval: A framework for comparative evaluation of explanation methods in healthcare. Digit Health. 2025;11:20552076251368045. DOI: 10.1177/20552076251368045</li>
<li>Qureshi MA, Noor AA, Manzoor A, et al. Explainability in Action: A Metric-Driven Assessment of Five XAI Methods for Healthcare Tabular Models. medRxiv. 2025 May 20;2025.05.20.25327976. DOI: 10.1101/2025.05.20.25327976</li>
<li>Liu S, McCoy AB, Peterson JF, et al. Leveraging explainable artificial intelligence to optimize clinical decision support. J Am Med Inform Assoc. 2024;31(4):968-974. DOI: 10.1093/jamia/ocae019</li>
<li>Noor AA, Manzoor A, Qureshi MDM, et al. Unveiling Explainable AI in Healthcare: Current Trends, Challenges, and Future Directions. Wiley Interdiscip Rev Data Min Knowl Discov. 2025;15(4):e70018. DOI: 10.1002/widm.70018</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://bishopbpharmd.github.io/Pharmacy-AI-Newsletter/">Pharmacy AI Newsletter</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
